# -*- coding: utf-8 -*-
"""
Created on Mon May 27 20:20:37 2024

@author: tmlab
"""




if __name__ == '__main__':
    
    import os
    import sys
    import pandas as pd
    import numpy as np     
    import warnings
    import pickle 
    from collections import Counter
    from openai import OpenAI
    client = OpenAI()
    
    
    #%% data load
    
    warnings.filterwarnings("ignore")
    
    # 서브모듈 경로 설정
    directory = os.path.dirname(os.path.abspath(__file__))
    directory = directory.replace("\\", "/") # window
    sys.path.append(directory+'/submodule')
    import preprocess
    
    
    # 위즈도메인 csv 파일이 존재하는 디렉토리 설정
    directory = os.environ['directory_path']
    directory += '열관리/'
    
    file_list = os.listdir(directory)
    file_list = [i for i in file_list if ".csv" in i ]
    
    data = pd.DataFrame()
    domain_dict = {}
    
    for file in file_list : 
        
        data_temp = pd.read_csv(directory + file, skiprows= 4)
        data_temp = preprocess.wisdomain_prep(data_temp)   
        domain_dict[file.split('.')[0]] = list(data_temp['id_wisdomain'])
        
        data = pd.concat([data, data_temp], axis = 0).reset_index(drop = 1)
    
    data = data.drop_duplicates(subset= ['id_wisdomain']).reset_index(drop = 1)
    
    #%% patent description scraping
    
    with open(directory + 'output/save1.pkl' ,"rb") as f :
        data_input = pickle.load(f)
        
    #%% for-fine tuning
    
    data_sample = data.sample(50, random_state=1234).reset_index(drop = 1)
    data_sample = data_sample[['id_wisdomain', 'abstract']]
    # data['abstract'][0]
    
    #%% generate smaple answer
    
    client = OpenAI(api_key= key)
    
    data_sample['answer'] = ""
    
    instruction= """Present results immediately without introduction
* Results are presented in json format.
* Results must be presented through extraction, and creation is prohibited."""

    for idx, row in data_sample.iterrows() : 
        print(idx)
    
        prompt = """Based on the perspectives below, Perform a functional classification of the following documents:
    
    1. Components: Individual parts or modules of the device.
    *For components, they must be presented in a tree structure.
    
    2. Functions: Specific operations performed by each component.
    * For functions, they must be presented as components connected by dependency relations.
    
    3. Conditions: Specific conditions or states under which certain functions are performed.
    
    4. Outputs: Results or outputs generated by the device or method.
    
    """      
        prompt += 'Q: In some embodiments, apparatuses and methods are provided herein useful to delivering climate controlled product. In some embodiments, there is provided a system for delivering climate controlled product via at least one autonomous unmanned aircraft system (UAS) that self-evaluates power sufficiency based on temperature tolerance of at least one product including: an autonomous UAS and at least one climate controlled product chamber. The UAS comprising: a control circuit, at least one rotor; a power supply, and a package coupler. The product chamber comprising: a chamber, at least one product reader, and a temperature control mechanism. The control circuit configured to: receive product identifier data; determine at least one climate threshold value; determine a confidence value of sufficient power remaining; compare whether the confidence value is within a risk threshold probability that a first mission will be completed; and initiate supply of power to the at least one rotor.'
        prompt += """\nA : {
          "Components": {
            "System": {
              "Autonomous UAS": {
                "Control Circuit": {},
                "Rotor": {},
                "Power Supply": {},
                "Package Coupler": {}
              },
              "Climate Controlled Product Chamber": {
                "Chamber": {},
                "Product Reader": {},
                "Temperature Control Mechanism": {}
              }
            }
          },
          "Functions": {
            "Control Circuit": [
              "Receive product identifier data",
              "Determine at least one climate threshold value",
              "Determine a confidence value of sufficient power remaining",
              "Compare whether the confidence value is within a risk threshold probability that a first mission will be completed",
              "Initiate supply of power to the at least one rotor"
            ]
          },
          "Conditions": [
            "Self-evaluation of power sufficiency based on temperature tolerance of at least one product"
          ],
          "Outputs": [
            "Climate controlled delivery of product",
            "Initiation of power supply to the rotor"
          ]
        }
    Q: """
    
        prompt += row['abstract']
        prompt += "\nA :"
    
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
            {"role": "user", 
             "content": prompt},
            {"role" : "system", "content" : instruction}
            ],temperature = 0.1)
        
        temp = completion.choices[0].message.content
        
        data_sample['answer'][idx] = temp    
    
    #%% fine-tuning 0
    import json 

    instruction = """Based on the perspectives below, Perform a functional classification of the following documents:

1. Components: Individual parts or modules of the device.
1.1. For components, they must be presented in a tree structure.

2. Functions: Specific operations performed by each component.
2.1. For functions, they must be presented as components connected by dependency relations.

3. Conditions: Specific conditions or states under which certain functions are performed.

4. Outputs: Results or outputs generated by the device or method.\n\n"""

    
    # 템플릿 정의
    instruction += """* Present results immediately without introduction
* Results are presented in json format.
* Results must be presented through extraction, and creation is prohibited.\n"""

    
    #%% fine-tuning1
    messages = []       
    
    for idx, row in data_sample.iterrows() : 
        
        print(idx)

        prompt = row['abstract']
        
        question_template = prompt
        
        response_template = row['answer']
        
        conversation_piece = {
            "messages": [
                {"role": "system", "content": instruction},
                {"role": "user", "content": question_template},
                {"role": "assistant", "content": response_template}
            ]
        }
        
        messages.append(conversation_piece)
    
    # 결과 출력
    print(json.dumps(messages, indent=2))          
    #%% fine-tuning data 2
    # 결과를 JSONL 파일로 저장
    file_path = os.environ['directory_path']+ '/gpt-finetuning/functional_classificaion_sample.jsonl'
    with open(file_path, 'w') as file:
        for message in messages:
            file.write(json.dumps(message) + '\n')
    
    #%% find functions in abstract by GPT
    
    data_input = data.loc[data["year_application"] >= 2010, : ].reset_index(drop = 1)
    data_input["abstract_tree"] = ""
    
    instruction ="""Based on the perspectives below, Perform a functional classification of the following documents:

1. Components: Individual parts or modules of the device.
1.1. For components, they must be presented in a tree structure.

2. Functions: Specific operations performed by each component.
2.1. For functions, they must be presented as components connected by dependency relations.

3. Conditions: Specific conditions or states under which certain functions are performed.

4. Outputs: Results or outputs generated by the device or method.

* Present results immediately without introduction
* Results are presented in json format.
* Results must be presented through extraction, and creation is prohibited."""

    for idx, row in data_input.iterrows() : 
        print(idx)
        
        if len(row["abstract_tree"]) == 0 :
            prompt = row['abstract']
        
            completion = client.chat.completions.create(
                model="ft:gpt-3.5-turbo-1106:snu:ptfunction:9eyA1SyW",
                messages=[
                {"role": "user", 
                 "content": prompt},
                {"role" : "system", "content" : instruction}
                ],temperature = 0.1)
            
            data_input["abstract_tree"][idx] = completion.choices[0].message.content
            

    #%% save
    import ast 
    # data_input.to_pickle(directory + 'output/save1.pkl')
    
    with open(directory + 'output/save1.pkl' ,"rb") as f :
        data_input = pickle.load(f)
    
    #%% preprocessing
    
    data_input["abstract_tree_"] = ""
    
    for idx, row in data_input.iterrows() :
        print(idx)
        abstract = row['abstract_tree']
        try : 
            abstract = ast.literal_eval(abstract)
            data_input["abstract_tree_"][idx] = abstract
        except : pass
        
        # data_input["abstract_tree"][idx]
    
    #%%
    
    data_input = data_input.loc[data_input["abstract_tree_"] != "", : ].reset_index(drop = 1)
    
    data_input["function"] = data_input["abstract_tree_"].apply(lambda x : x["Functions"])
    data_input["function_list"] = ""
    
    for idx, row in data_input.iterrows() : 
        function = row["function"]
        function_list = []
        
        for k,v in function.items() : 
        
            for func in v : 
                function_list.append(func)
                
        data_input["function_list"][idx] = function_list
        
    #%% zero-shot classification - thermal management
        
    from transformers import pipeline

    classifier = pipeline("zero-shot-classification",
                          model = "MoritzLaurer/deberta-v3-large-zeroshot-v2.0",
                          device = 0) # https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0

    label_descriptions = {

        "Heat management" : "Heat+[management or control or regulation or moderation or adjustment or stabilization or dissipation]",
        "Thermal management" : "Thermal+[management or control or regulation or moderation or adjustment or stabilization or dissipation]",
        "Temperature management" : "Temperature+[management or control or regulation or moderation or adjustment or stabilization or dissipation]",
        
        }
    
    labels = list(label_descriptions.keys())
    paragraph = "CFirst Temperature Sensor: Measures a first temperature of hydraulic fluid upstream of an inlet of the pump"
    classifier(paragraph, 
               list(label_descriptions.values()), 
               multi_label=True)['scores']
    
    
    #%%
    data_input['HM'] = [[]]*len(data_input)
    
    for idx, row in data_input.iterrows() :
        
        print(idx)
        
        if len(row['HM']) == 0 :
            summary = row['function_list']
            temp = []
            
            for paragraph in summary : 
                if len(paragraph) >= 20 :
                    score = max(
                        classifier(paragraph, 
                                   list(label_descriptions.values()), 
                                   multi_label=True)['scores'])
                
                    if score >= 0.3 : 
                        temp.append(paragraph)
                    
                    else : print(paragraph)
            
            data_input['HM'][idx] = temp
    
    #%% save1
    
    data_input.to_pickle(directory + 'output/save2.pkl')
    
    
    #%% test
    temp = data_input[['id_wisdomain','function_list', 'HM']]
    
    temp['Not_HM'] = temp.apply(lambda x : [i for i in x.function_list if i not in x.HM], axis= 1)
    
    # with open(directory + 'output/save1.pkl' ,"rb") as f :
        # data_input = pickle.load(f)
    
    #%%
    
    c = Counter(data_input['year_application'])
    docs_HM = pd.DataFrame()
    
    for idx, row in data_input.iterrows() : 
        print(idx)
        
        paragraphs = row['HM']
        
        for paragraph in paragraphs : 
            temp = pd.DataFrame({'document' : [paragraph],
                                 'pt_id' : [row['id_wisdomain']],
                                 'year' : [row['year_application']]})
            docs_HM = pd.concat([docs_HM, temp], axis = 0).reset_index(drop = 1)
       
    
    docs_HM_series = []

    for year in range(2010, 2023, 2) :
        
        print(year)
        
        data_temp = docs_HM.loc[(docs_HM['year'] >= year) , :] 
        # if year == 202
        data_temp = data_temp.loc[(data_temp['year'] < year+2) , :].reset_index(drop = 1)
        
        # temp = data_temp['document']
        
        docs_HM_series.append(data_temp)
    
    #%% incremental clustering
    
    from river import stream
    from river import cluster
    from bertopic.vectorizers import ClassTfidfTransformer
    from bertopic.vectorizers import OnlineCountVectorizer
    from bertopic import BERTopic
    
    class River:
        
        def __init__(self, model):
            self.model = model

        def partial_fit(self, umap_embeddings):
            for umap_embedding, _ in stream.iter_array(umap_embeddings):
                self.model = self.model.learn_one(umap_embedding)

            labels = []
            for umap_embedding, _ in stream.iter_array(umap_embeddings):
                label = self.model.predict_one(umap_embedding)
                labels.append(label)

            self.labels_ = labels
            return self


    
    #%% initialize
    
    data_input['domain'] = [[]]*len(data_input)
    
    for idx, row in data_input.iloc[:,:].iterrows() : 
        
        if len(row['domain']) == 0 :
            temp = []
        else : 
            temp = row['domain']
        # print(idx)
        Id_wisdomain = row['id_wisdomain']
        
        for key in domain_dict.keys() :
            if Id_wisdomain in domain_dict[key] :
                temp.append(key)

        data_input['domain'][idx] = temp

    
    #%% Incrementally fit the topic model by training on documents at a time
    from sentence_transformers import SentenceTransformer
    from umap import UMAP
    # Using DBSTREAM to detect new topics as they come in
    cluster_model = River(cluster.DBSTREAM(
        
        # clustering_threshold = 0.75, #default 1.0, 커지면 마이크로 클러스터 생성 x
        # fading_factor = 0.02,  #0.01, 과거 데이터의 중요도 결정
        # intersection_factor= 0.3, #0.3 커질수록 주제수가 줄어든다
        # minimum_weight= 2.0
        ))
  
    vectorizer_model = OnlineCountVectorizer(stop_words="english", 
                                             # decay = 0.1,
                                             delete_min_df = 2,
                                             
                                             )
    umap_model = UMAP(n_neighbors=15, 
                      n_components=5, 
                      min_dist=0.0, 
                      metric='cosine')
    
    ctfidf_model = ClassTfidfTransformer(
        # reduce_frequent_words=True, 
        # bm25_weighting=True,
        )
    
    embedding_model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa_v2')
    
    # Prepare model
    topic_model = BERTopic(
        embedding_model=embedding_model,
        hdbscan_model=cluster_model, 
        vectorizer_model=vectorizer_model, 
        ctfidf_model=ctfidf_model,
        umap_model=umap_model,
        calculate_probabilities=True,
    )


    result_dynamic = {}
    
    year = 2010
    gap = 2
    idx_save = 0 
    
    for idx, data_temp in enumerate(docs_HM_series[:]) :
        
        documents = data_temp['document']
        
        print(year)
        
        topic_model.partial_fit(documents)
        
        result_topic_info = topic_model.get_topic_info()
        result_document_info = topic_model.get_document_info(documents) # 40개의 도메인
        
        result_dynamic[year] = result_topic_info
        
        count = len(documents)
        
        data_temp['Topic'] = result_document_info['Topic']
        docs_HM_series[idx] = data_temp 
        
        idx_save += count
        year += gap
        
    # best parameter?
            
    data_input['topic'] = [[]]*len(data_input)
    data_input['time'] = 0
    
    for idx, data_temp in enumerate(docs_HM_series[:]) : 
        
        for idx_, row in data_temp.iterrows() : 
            
            pt_id = row["pt_id"]
            topic = row["Topic"]
            id_idx = data_input.loc[data_input["id_wisdomain"] == pt_id, :].index[0]
            
            temp = data_input['topic'][id_idx]
            
            if len(temp) == 0 :
                temp = [topic]
            else : 
                temp.append(topic)
            
            data_input['topic'][id_idx] = temp
            data_input['time'][id_idx]= idx
       
    
    #%% domain 치환
    
    temp = pd.DataFrame(zip(domain_dict.keys(), domain_dict.values()))
    temp = temp.explode(1)
    temp = temp.drop_duplicates(keep = False)
    domain_dict_reverse = dict(zip(temp[1],temp[0]))
    # for df in docs_HM_series :
        
    data_input['domain'] = data_input.apply(lambda x : domain_dict_reverse[x.id_wisdomain], axis=1)
    
    data_input['topic'] = [list(set(i)) for i in data_input['topic']]
    
    from sklearn.metrics import adjusted_rand_score
    
    data_input_exploded = data_input.explode("domain")
    data_input_exploded = data_input_exploded.explode("topic")
    
    data_input_exploded = data_input_exploded.dropna(subset = ["topic"])
    
    print(adjusted_rand_score(data_input_exploded["domain"], data_input_exploded["topic"])) # 0.045 #0.11 
        
    #%% cross-table
    
    contingency_table = pd.crosstab(data_input_exploded['topic'], data_input_exploded['domain'])
    filtering_rows = contingency_table[contingency_table.sum(axis = 1) < 5].index.tolist()
    
    # temp = result_topic_info.loc[result_topic_info["Topic"].isin(filtering_rows), : ]
    
    proportional_by_row = contingency_table.div(contingency_table.sum(axis = 1), axis = 0)
    rows_greater_than_75 = proportional_by_row.max(axis=1) > 0.75
    # index_temp = proportional_by_row[rows_greater_than_75].index.tolist()
    # index_temp = [i for i in index_temp if i not in temp.index]
    # temp = result_topic_info.loc[result_topic_info["Topic"].isin(index_temp), : ]
    
    filtering_rows.extend(proportional_by_row[rows_greater_than_75].index.tolist()) 
    
    filtering_rows = list(set(filtering_rows))
    # filtering results
    
    result_dynamic_filtered = {}
    result_dynamic_generation = {}
    topics = []
    
    for k, v in result_dynamic.items() : 
        v = v.loc[~v["Topic"].isin(filtering_rows)]
        result_dynamic_filtered[k] = v
        
        if k != 2010 : 
            topics = list(result_dynamic_filtered[k-2]["Topic"])
            v = v.loc[v["Topic"].isin(topics)]
            result_dynamic_generation[k] = v
            
    # result_topic_info = topic_model.get_topic_info()
    # result_document_info = topic_model.get_document_info(docs_flatten) # 40개의 도메인
    
    #%% 시각화를 위한 번역
    translate_dict = {"항공" : 'Aircraft',
                      "서버" : "Server", 
                      "EVB" : "EVB", 
                      "HVAC" : "HVAC",
                      "원자력" : "Nuclear Energy",
                      "우주" : "Spacecraft",
                      "국방" : "Defense", 
                      "반도체_포장" : "Semiconductor Package"}
    
    data_input_exploded["domain"] = data_input_exploded["domain"].apply(lambda x : translate_dict[x])
    
    #%% docs_HM에 domain 할당 및 embedding
    
    for idx,df in enumerate(docs_HM_series) :
        df['domain'] = df['pt_id'].apply(lambda x : domain_dict_reverse[x])
        
        
        temp = embedding_model.encode(df['document'])
        df['vector'] = [i for i in temp]
        df['domain'] = df['domain'].apply(lambda x : translate_dict[x])
        docs_HM_series[idx]= df
    
    #%%
    docs_HM_series_cumulative = []
    
    temp = pd.DataFrame()
    
    for idx, row in enumerate(docs_HM_series) : 
        row = row.loc[~row['Topic'].isin(filtering_rows),:].reset_index(drop = 1)
        temp = pd.concat([temp, row], axis = 0)
        
        docs_HM_series_cumulative.append(temp)
    
    #%%
    temp = docs_HM_series_cumulative[6].groupby(["domain"]).count()
    
    #%%
    
    data_temp = data_input_exploded.loc[data_input_exploded["domain"] == "EVB", :]
    
    from sklearn.metrics.pairwise import cosine_similarity
    from copy import copy
    
    contingency_table_original = pd.crosstab(data_temp['topic'], data_temp['time'])    
    indcies_original = [i for i in contingency_table_original.index if i not in filtering_rows]
    contingency_table_original = contingency_table_original.loc[indcies_original]    
    contingency_table_original = contingency_table_original.cumsum(axis=1)
    references = list(translate_dict.values())
    references.remove("EVB")
    
    result_VO = pd.DataFrame()
    
    
    for refer in references : 
        data_temp = data_input_exploded.loc[data_input_exploded["domain"] == refer, :]
        
        contingency_table_reference = pd.crosstab(data_temp['topic'], data_temp['time'])
        indcies = [i for i in contingency_table_reference.index if i not in filtering_rows]
        indcies = [i for i in indcies if i in indcies_original]
        
        contingency_table_reference = contingency_table_reference.loc[indcies]
        
        VO_matrix = pd.DataFrame()
        
        for time in range(0,6) :
            #VO
            VO = (contingency_table_reference[time]).div(contingency_table_original[time], fill_value= 0)
            VO = VO.replace([np.inf,-np.inf, np.nan], 0)
            VO = np.log2(VO+1) # 직전 시점
            VO = pd.DataFrame(VO)
            
            VO_matrix = pd.concat([VO_matrix, VO], axis= 1)
            
        VO_matrix.columns = range(0,6)
        
        for col in VO_matrix.columns :
            for idx in VO_matrix.index :
                time = col
                topic = idx
                temp = pd.DataFrame.from_dict({
                    "reference" : [refer],
                    "time" : [time], 
                    "topic" : [topic],
                    "VO" : [VO_matrix[col][idx]]})
        
                result_VO = pd.concat([result_VO, temp], axis = 0)
         
    
    # Adaptability
    result_VO = result_VO.reset_index(drop = 1)
    A_original_df = pd.DataFrame()
    
    result_VO['A'] = 0
    
    for idx,row in result_VO.iterrows() :
        print(idx)
        
        if row['VO'] == 0 : continue
    
        time = row['time']
        topic = row['topic']
        refer = row['reference']
        
        A_original_df = docs_HM_series_cumulative[time]
        A_original_df = A_original_df.loc[(A_original_df["domain"] == "EVB"), :]
        A_original_df = A_original_df.loc[(A_original_df["Topic"] == topic), :]
        
        A_reference_df = docs_HM_series_cumulative[time]
        A_reference_df = A_reference_df.loc[(A_reference_df["domain"] == refer), :]
        A_reference_df = A_reference_df.loc[(A_reference_df["Topic"] == topic), :]
        
        if any([len(A_original_df)==0, len(A_reference_df)==0 ])  : 
            continue
        
        centroid_o = np.mean(A_original_df['vector'])
        centroid_r = np.mean(A_reference_df['vector'])
        sim = cosine_similarity([centroid_o], [centroid_r])[0][0]
        print(sim)
        result_VO['A'][idx] = sim
        
    # FD
    
    result_VO['FD'] = 0
    
    for idx,row in result_VO.iterrows() :
        
        time = row['time']
        topic = row['topic']
        
        result_VO['FD'][idx] = contingency_table_original[time+1][topic]-contingency_table_original[time][topic]
        # result_VO['FD'][idx] = sim
        # result_VO['A'] = 0
    
    #
    
    result_VO = result_VO.sort_values(by='topic')    
    result_VO['TO'] = result_VO.apply(lambda x : x.VO* x.A* x.FD, axis = 1)
    #%% VO visualize
    
    import seaborn as sns
    import matplotlib.pyplot as plt
    import numpy as np
    
    
    result_VO['topic'] = result_VO['topic'].apply(lambda x : str(x))
    
    df= result_VO.loc[result_VO["VO"] > 1 , :]
    
    
    # Create a mask for elements greater than or equal to 1    
    g = sns.relplot(x = "topic", y = "time", 
                    size = "VO",
                    hue = "reference",
                    
                    sizes = (0,700),
                    
                    marker = "s",
                    legend = "brief", 
                    aspect = 5,
                    height = 3,

                    data= df)
    
    g.ax.grid(True)  # Add grid
    g.ax.invert_yaxis()
    
    #%% TO visualize
    
    df = copy(result_VO)
    df = df.sort_values(by = ['time','topic'] )
    df['topic'] = df['topic'].apply(lambda x : str(x))
    df['time'] = df['time'].apply(lambda x : str(2010+x*2))
    
    
    df = df.loc[df["VO"] > 1 , :]
    # df= df.loc[df["time"].isin(["0","1","2"]), :]
    # df= df.loc[df["time"].isin(["3","4","5"]), :]
    
    # Create a mask for elements greater than or equal to 1    
    g = sns.relplot(x = "topic", y = "time", 
                    size = "TO",
                    hue = "reference",
                    
                    sizes = (0,700),
                    marker = "s",
                    legend = "brief", 
                    aspect = 3,
                    height = 5,


                    data= df)
    # sns.set(font_scale= )  # crazy big
    
    g.ax.grid(True)  # Add grid
    # g.ax.invert_yaxis()
    
    #%% What patent ?
    
    A_reference_df = docs_HM_series_cumulative[0]
    A_reference_df = A_reference_df.loc[(A_reference_df["domain"] == "Server"), :]
    A_reference_df = A_reference_df.loc[(A_reference_df["Topic"] == 9), :]
    
    #%% Best field?
    
    temp = result_VO.groupby(['reference','topic'])["TO"].sum()
    
    temp = temp[temp>1]
    temp = temp.reset_index(drop = 0)
    
    temp['topic'] = temp['topic'].apply(lambda x : int(x))
    temp = temp.sort_values("topic")
    
    temp = temp.pivot(index= "reference", columns = "topic", values = "TO")
    
    # Plotting the data
    plt.figure(figsize=(20,5))
    sns.heatmap(temp, annot=True, fmt=".1f", cmap="Blues", linewidths=.5)
    plt.title('TO Values Heatmap by Reference and Topic')
    plt.xlabel('Reference')
    plt.ylabel('Topic')
    plt.show()
    
    #%% Best field? 2 
    
    temp = result_VO.groupby(['topic'])["TO"].sum()
    
    #%%
    prompt = """
I have topic that contains the following documents: \n[DOCUMENTS]
The topic is described by the following keywords: ['compressor', 'vehicle', 'cell', 'battery', 'valve', 'fluid', 'heating', 'exhaust', 'engine', 'refrigerant']

Based on the above information, can you give a short label of the topic?
"""

    #%%
    from anytree import Node, RenderTree
    from anytree.exporter import DotExporter
    import os
    
    def dict_to_tree(dictionary, root_name):
        root = Node(root_name)
        
        def add_nodes(parent, dictionary):
            for key, value in dictionary.items():
                if isinstance(value, dict):
                    child = Node(key, parent=parent)
                    add_nodes(child, value)
                else:
                    Node(f"{key}: {value}", parent=parent)
        
        add_nodes(root, dictionary)
        return root
    
    def visualize_tree(dictionary, root_name="root"):
        tree_root = dict_to_tree(dictionary, root_name)
        
        # Print the tree to console
        for pre, fill, node in RenderTree(tree_root):
            print(f"{pre}{node.name}")
        
        # Export to a .dot file for visualization
        try:
            DotExporter(tree_root).to_picture(directory + "/tree.png")
            print("Tree visualization saved as tree.png")
        except FileNotFoundError as e:
            print("Graphviz executables not found. Please ensure Graphviz is installed and added to your PATH.")
            raise e
    

    
    #%%
    # Visualize the dictionary as a tree
    visualize_tree(data_input['abstract_tree_'][2]['Components'],
                   data_input['id_wisdomain'][2])

    #%%
    temp = data_input['abstract_tree_'][2]
    #%%
    # temp = topic_model.get_topics()
    temp_ = topic_model.get_topics()

    #%% summary analysis
    
    import collections 
    for idx, row in data.iterrows() : 
        if type(row['description_']) != type  : 
            
            keys = list(row['description_'].keys())
            
            if 'SUMMARY' in keys : 
                
                # data['summary'][idx] = "\n".join(row['description_']['SUMMARY'])
                data['summary'][idx] = row['description_']['SUMMARY']
                
        else : pass
    #%% 
    import openai
    from openai import OpenAI
    
    docs = list(data['abstract'])
    
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    client = OpenAI()
    prompt = "Split this document into paragraphs : \n"
    prompt += docs[20]
    
    import ast
    instruction = 'Present the results in the form of a Python list. For example, ["paragraph A", "paragraph B", ... "paragraph X"]'
    
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
        {"role": "user", 
         "content": prompt},{"role" : "system", "content" : instruction}
        ])
    
    print(completion.choices[0].message)
    
    #%%
    temp = completion.choices[0].message.content
    temp = ast.literal_eval(temp)
    #%%
    split_sentences = split_text(docs[-113], delimiters)
    
    docs = [split_text(i, delimiters) for i in docs]
    
    
    #%%
    data['sentence_list'] = docs
    
    #%% zero-shot-classfication
    
    #%%    

    
    #%%
    
    candidate_labels = ["electric vehicle", "military defense", "photovoltaic cells",
                         "aircraft", "spacecraft", "data centers"]
    
        
    #%%
    
    #%%
    sequence_to_classify = "There is provided a vehicle system, including an electromobile having a fuel cell power generator mounted therein, and a predetermined mobile unit jointable to the electromobile, such that cogeneration within the mobile unit can be realized even during the travel of the electromobile."
    classifier(sequence_to_classify, candidate_labels)
    classifier(sequence_to_classify, candidate_labels, multi_label=True)
    
    
    #%%
    sequence_to_classify = "one day I will see the world"
    candidate_labels = ['travel', 'cooking', 'dancing']

    #%% example 2
    
    candidate_labels = ['semiconductor', 
                        'electric vehicle',
                        "thermal management",]
    
    
    # sequence_to_classify = "A battery system can provide backup power for information technology (IT) equipment."
    # sequence_to_classify = "In response to a lithium ion based battery being inactive (not charging or discharging), a temperature of the battery can be maintained at or below an optimal storage temperature of the battery, using a primary cooling system."
            
    classifier(sequence_to_classify, candidate_labels, multi_label=True)
    
    #%% example 3
    # Candidate labels with their descriptions
    
    label_descriptions = {
        # "Heat management" : 
        # "Cooling" : "Cooling is removal of heat, usually resulting in a lower temperature and/or phase change.",
        # "Coolant" : "A coolant is a substance, typically liquid, that is used to reduce or regulate the temperature of a system",
        # "Phase-change material": "A phase-change material (PCM) is a substance which releases/absorbs sufficient energy at phase transition to provide useful heat or cooling. Generally the transition will be from one of the first two fundamental states of matter - solid and liquid - to the other. The phase transition may also be between non-classical states of matter, such as the conformity of crystals, where the material goes from conforming to one crystalline structure to conforming to another, which may be a higher or lower energy state.",
        "Heat sink" : "A heat sink (also commonly spelled heatsink,[1]) is a passive heat exchanger that transfers the heat generated by an electronic or a mechanical device to a fluid medium, often air or a liquid coolant, where it is dissipated away from the device, thereby allowing regulation of the device's temperature. In computers, heat sinks are used to cool CPUs, GPUs, and some chipsets and RAM modules. Heat sinks are used with other high-power semiconductor devices such as power transistors and optoelectronics such as lasers and light-emitting diodes (LEDs), where the heat dissipation ability of the component itself is insufficient to moderate its temperature.",
        "Heat pipe" : "A heat pipe is a heat-transfer device that employs phase transition to transfer heat between two solid interfaces.",
        "PCM" : "A phase-change material (PCM) is a substance which releases/absorbs sufficient energy at phase transition to provide useful heat or cooling.",
        
        "Semiconductor" : "A semiconductor is a material that has an electrical conductivity value falling between that of a conductor, such as copper, and an insulator, such as glass. Its resistivity generally falls as its temperature rises; metals behave in the opposite way.",
    }
    
    labels = list(label_descriptions.keys())
    
    
    sentences = [
    "What is claimed is:",
    "1. An assembly for charging a traction battery of a vehicle comprising: a charge port configured to conductively transfer charging current from an external source to the vehicle; and a cooling system configured to cool the charge port depending on a temperature of the charge port.",
    "2. The assembly of claim 1 wherein: the cooling system includes a temperature sensor configured to monitor the temperature of the charge port.",
    "3. The assembly of claim 1 wherein: the cooling system is a liquid cooling system configured to use coolant to cool the charge port.",
    "4. The assembly of claim 3 wherein: the liquid cooling system includes a cold plate having a coolant chamber with a coolant ingress port and a coolant egress port, the cold plate being attached to the charge port.",
    "5. The assembly of claim 4 wherein: the liquid cooling system further includes a heat exchanger configured to cool heated coolant, the liquid cooling system is further configured to provide a flow of coolant through the coolant chamber of the cold plate for absorbing heat from the charge port and to the heat exchanger.",
    "6. The assembly of claim 1 wherein: the cooling system is an air cooling system configured to use air to cool the charge port.",
    "7. The assembly of claim 6 wherein: the air cooling system is a forced air cooling system configured to provide a forced flow of air toward the charge port to dissipate heat from the charge port into an environment of the charge port.",
    "8. The assembly of claim 6 wherein: the air cooling system is a natural air convection system.",
    "9. The assembly of claim 8 wherein: the natural air convection system includes a heat-sink device attached to the charge port to absorb heat of the charge port and dissipate the heat into an environment of the charge port.",
    "10. The assembly of claim 9 wherein: the heat-sink device includes at least one of a heat pipe, heat spreader, and a thermoelectric cooling module.",
    "11. The assembly of claim 1 wherein: the charge port is further configured to mate with a charge connector associated with the external source to conductively transfer charging current from the external source to the vehicle."]

    # Text to classify
    text = "Said heat dissipation structure can significantly reduce the temperature of the hot end of the thermoelectric heat sink (4), so that the temperature of the LED chip (1) is actively reduced, directly by means of the thermoelectric heat sink (4), said heat dissipation structure has a good effect, and can improve the working performance, reliability and service life of the LED."
    text = "a thermal storage medium positioned about and immediately surrounding the rechargeable vehicle battery, the thermal storage medium configured to receive and store heat produced by charging the rechargeable vehicle battery, such that the first coolant absorbs the heat stored in the thermal storage medium, wherein the thermal storage medium is a phase change material; and"
    
    # Perform zero-shot classification using label descriptions as candidate labels
    result = classifier(text, 
                        list(label_descriptions.values()), 
                        multi_label=True
                        )
    
    # Match scores with original labels
    scores_with_labels = dict(zip(labels, result['scores']))
    
    
    scores_with_labels
    
    